<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on [R]eliability</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on [R]eliability</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019. All rights reserved.</copyright>
    <lastBuildDate>Sun, 01 Dec 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title> Creating and Using a Simple, Bayesian Linear Model (in brms and R)</title>
      <link>/post/creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post is my good-faith effort to create a simple linear model using the Bayesian framework and workflow described by Richard McElreath in his Statistical Rethinking book.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; As always - please view this post through the lens of the eager student and not the learned master. I did my best to check my work, but it’s entirely possible that something was missed. Please let me know - I won’t take it personally. As McElreath notes in his lectures - “if you’re confused, it’s because you’re paying attention”. And sometimes I get confused - this a lot harder than my old workflow which consisted of clicking “add a trendline” in Excel. Thinking Bayesian is still relatively new to me. Disclaimer over - let’s get to it.&lt;/p&gt;
&lt;p&gt;I’m playing around with a bunch of fun libraries in this one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(styler)
library(ggExtra)
library(knitr)
library(brms)
library(cowplot)
library(gridExtra)
library(skimr)
library(DiagrammeR)
library(rayshader)
library(av)
library(rgl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I made up this data set. It represents hypothetical values of ablation time and tissue impedance as measured by sensors embedded in a RF ablation catheter. This type of device is designed to apply RF or thermal energy to the vessel wall. The result is a lesion that can aid in improve arrhythmia, reduce hypertension, or provide some other desired outcome.&lt;/p&gt;
&lt;p&gt;In RF ablations, the tissue heats up over the course of the RF cycle, resulting in a drop in impedance that varies over time. As described above, the goal will be to see how much of the variation in impedance is described by time (over some limited range) and then communicate the uncertainty in the predictions visually. None of this detail is terribly important other than I like to frame my examples from within my industry and McElreath emphasizes grounding our modeling in real world science and domain knowledge. This is what an ablation catheter system looks like:&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./img/rf_cath.jpg&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To get things started, load the data and give it a look with skim(). There are no missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ablation_dta_tbl &amp;lt;- read.csv(file = &amp;quot;abl_data_2.csv&amp;quot;)
ablation_dta_tbl &amp;lt;- ablation_dta_tbl %&amp;gt;% select(temp, time)
ablation_dta_tbl %&amp;gt;% skim()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 331 
##  n variables: 2 
## 
## -- Variable type:numeric ---------------------------------------------------------------------------------------------------------------
##  variable missing complete   n  mean   sd    p0   p25   p50   p75  p100
##      temp       0      331 331 77.37 3.9  68.26 74.61 77.15 80.33 89.53
##      time       0      331 331 22.57 3.22 15.83 20.22 22.54 24.69 31.5 
##      hist
##  &amp;lt;U+2581&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2586&amp;gt;&amp;lt;U+2586&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;
##  &amp;lt;U+2582&amp;gt;&amp;lt;U+2586&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start with a simple visualization. The code below builds out a scatterplot with marginal histograms which I think is a nice, clean way to evaluate scatter data.&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; These data seem plausible since the impedance will typically drop as the tissue heats up during the procedure. In reality the impedance goes asymptotic but we’ll work over a limited range of time where the behavior might reasonably be linear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scatter_1_fig &amp;lt;- ablation_dta_tbl %&amp;gt;% ggplot(aes(x = time, y = temp)) +
  geom_point(
    colour = &amp;quot;#2c3e50&amp;quot;,
    fill = &amp;quot;#2c3e50&amp;quot;,
    size = 2,
    alpha = 0.4
  ) +
  labs(
    x = &amp;quot;Ablation Time (seconds)&amp;quot;,
    y = &amp;quot;Tissue Temperature (deg C)&amp;quot;,
    title = &amp;quot;Ablation Time vs. Tissue Temperature&amp;quot;,
    subtitle = &amp;quot;Simulated Catheter RF Ablation&amp;quot;
  )

scatter_hist_1_fig &amp;lt;- ggMarginal(scatter_1_fig,
  type = &amp;quot;histogram&amp;quot;,
  color = &amp;quot;white&amp;quot;,
  alpha = 0.7,
  fill = &amp;quot;#2c3e50&amp;quot;,
  xparams = list(binwidth = 1),
  yparams = list(binwidth = 2.5)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggExtra needs these explit calls to display in Markdown docs *shrug*
grid::grid.newpage()
grid::grid.draw(scatter_hist_1_fig)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-01-creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It helps to have a plan. If I can create a posterior distribution that captures reasonable values for the model parameters and confirm that the model makes reasonable predictions then I will be happy. Here’s the workflow that hopefully will get me there.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grViz(&amp;quot;digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle, fillcolor = yellow]        
      tab1 [label = &amp;#39;Step 1: Propose a distribution for the response variable \n Choose a maximum entropy distribution given the constraints you understand&amp;#39;]
      tab2 [label = &amp;#39;Step 2: Parameterize the mean \n The mean of the response distribution will vary linearly across the range of predictor values&amp;#39;]
      tab3 [label = &amp;#39;Step 3: Set priors \n Simulate what the model knows before seeing the data.  Use domain knowledge as constraints.&amp;#39;]
      tab4 [label = &amp;#39;Step 4: Define the model \n Create the model using the observed data, the likelihood function, and the priors&amp;#39;]
      tab5 [label = &amp;#39;Step 5: Draw from the posterior \n Plot plausible lines using parameters visited by the Markov chains&amp;#39;]
      tab6 [label = &amp;#39;Step 6: Push the parameters back through the model \n Simulate real data from plausible combinations of mean and sigma&amp;#39;]
      # edge definitions with the node IDs
      tab1 -&amp;gt; tab2 -&amp;gt; tab3 -&amp;gt; tab4 -&amp;gt; tab5 -&amp;gt; tab6;
      }
      &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph flowchart {\n      # node definitions with substituted label text\n      node [fontname = Helvetica, shape = rectangle, fillcolor = yellow]        \n      tab1 [label = \&#34;Step 1: Propose a distribution for the response variable \n Choose a maximum entropy distribution given the constraints you understand\&#34;]\n      tab2 [label = \&#34;Step 2: Parameterize the mean \n The mean of the response distribution will vary linearly across the range of predictor values\&#34;]\n      tab3 [label = \&#34;Step 3: Set priors \n Simulate what the model knows before seeing the data.  Use domain knowledge as constraints.\&#34;]\n      tab4 [label = \&#34;Step 4: Define the model \n Create the model using the observed data, the likelihood function, and the priors\&#34;]\n      tab5 [label = \&#34;Step 5: Draw from the posterior \n Plot plausible lines using parameters visited by the Markov chains\&#34;]\n      tab6 [label = \&#34;Step 6: Push the parameters back through the model \n Simulate real data from plausible combinations of mean and sigma\&#34;]\n      # edge definitions with the node IDs\n      tab1 -&gt; tab2 -&gt; tab3 -&gt; tab4 -&gt; tab5 -&gt; tab6;\n      }\n      &#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;step-1-propose-a-distribution-for-the-response-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Step 1: Propose a distribution for the response variable&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A Gaussian model is reasonable for the outcome variable Temperature as we know it is a measured from the thermocouples on the distal end of the catheter. According to McElreath (pg 75):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here’s us formally asserting Temperature as a normal distribution with mean and standard deviation . These two parameters are all that is needed to completely describe the distribution and also pin down the likelihood function.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T_i \sim \text{Normal}(\mu_i, \sigma)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-parameterize-the-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Step 2: Parameterize the mean&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If we further parameterize , we can do some neat things like move the mean around with the predictor variable. This is a pretty key concept - &lt;em&gt;you move the mean of the outcome variable around by parameterizing it. If we make it a line then it will move linearly with the predictor variable.&lt;/em&gt; The real data will still have a spread once the terms is folded back in, but we can think of the whole distribution shifting up and down based on the properties of the line.&lt;/p&gt;
&lt;p&gt;Here’s us asserting we want mu to move linearly with changes in the predictor variable (time). Subtracting the mean from each value of the predictor variable “centers” the data which McElreath recommends in most cases. I will explore the differences between centered and un-centered later on.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_i = \alpha + \beta (x_i - \bar{x})\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-set-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Step 3: Set priors&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We know some things about these data. Temperature is a continuous variable so we want a continuous distribution. We also know from the nature of the treatment that there isn’t really any physical mechanism within the device that would be expected to cool down the tissue below normal body temperature. Since only heating is expected, the slope should be positive or zero.&lt;/p&gt;
&lt;p&gt;McElreath emphasizes simulating from the priors to visualize “what the model knows before it sees the data”. Here are some priors to consider. Let’s evaluate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set seed for repeatability
set.seed(1999)

# number of sims
n &amp;lt;- 150

# random draws from the specified prior distributions
# lognormal distribution is used to constrain slopes to positive values
a &amp;lt;- rnorm(n, 75, 15)

b &amp;lt;- rnorm(n, 0, 1)
b_ &amp;lt;- rlnorm(n, 0, 0.8)

# calc mean of time and temp for later use
mean_temp &amp;lt;- mean(ablation_dta_tbl$temp)
mean_time &amp;lt;- mean(ablation_dta_tbl$time)

# dummy tibble to feed ggplot()
empty_tbl &amp;lt;- tibble(x = 0)

# y = b(x - mean(var_1)) + a is equivalent to:
# y = bx + (a - b * mean(var_1))

# in this fig we use the uninformed prior that generates some unrealistic values
prior_fig_1 &amp;lt;- empty_tbl %&amp;gt;% ggplot() +
  geom_abline(
    intercept = a - b * mean_time,
    slope = b,
    color = &amp;quot;#2c3e50&amp;quot;,
    alpha = 0.3,
    size = 1
  ) +
  ylim(c(0, 150)) +
  xlim(c(0, 150)) +
  labs(
    x = &amp;quot;time (sec)&amp;quot;,
    y = &amp;quot;Temp (C)&amp;quot;,
    title = &amp;quot;Prior Predictive Simulations&amp;quot;,
    subtitle = &amp;quot;Uninformed Prior&amp;quot;
  )

# in this fig we confine the slopes to broad ranges informed by what we know about the domain
prior_fig_2 &amp;lt;- empty_tbl %&amp;gt;% ggplot() +
  geom_abline(
    intercept = a - b_ * mean_time,
    slope = b_,
    color = &amp;quot;#2c3e50&amp;quot;,
    alpha = 0.3,
    size = 1
  ) +
  ylim(c(0, 150)) +
  xlim(c(0, 150)) +
  labs(
    x = &amp;quot;time (sec)&amp;quot;,
    y = &amp;quot;Temp (C)&amp;quot;,
    title = &amp;quot;Prior Predictive Simulations&amp;quot;,
    subtitle = &amp;quot;Mildly Informed Prior&amp;quot;
  )

plot_grid(prior_fig_1, prior_fig_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-01-creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plots above show what the model thinks before seeing the data for two different sets of priors. In both cases, I have centered the data by subtracting the mean of the time from each individual value of time. This means the intercept has the meaning of the expected temperature at the mean of time. The family of lines on the right seem a lot more realistic despite having some slopes that predict strange values out of sample (blood coagulates at ~90C). Choosing a log normal distribution for time ensures positives slopes. You could probably go even tighter on these priors but for this exercise I’m feeling good about proceeding.&lt;/p&gt;
&lt;p&gt;Looking only at the time window of the original observations and the Temp window bounded by body temperature (lower bound) and water boiling (upper bound).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;empty_tbl %&amp;gt;% ggplot() +
  geom_abline(
    intercept = a - b_ * mean_time,
    slope = b_,
    color = &amp;quot;#2c3e50&amp;quot;,
    alpha = 0.3,
    size = 1
  ) +
  ylim(c(37, 100)) +
  xlim(c(15, 40)) +
  labs(
    x = &amp;quot;time (sec)&amp;quot;,
    y = &amp;quot;Temp (C)&amp;quot;,
    title = &amp;quot;Prior Predictive Simulations&amp;quot;,
    subtitle = &amp;quot;Mildly Informed Prior, Original Data Range&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-01-creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here are the prior distributions selected to go forward.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha \sim \text{Normal}(75, 15)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta \sim \text{LogNormal}(0, .8)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma \sim \text{Uniform}(0, 30)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-define-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Step 4: Define the model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Here I use the brm() function in brms to build what I’m creatively calling: “model_1”. This one uses the un-centered data for time. This function uses Markov Chain Monte Carlo to survey the parameter space. After the warm up cycles, the relative amount of time the chains spend at each parameter value is a good approximation of the true posterior distribution. I’m using a lot of warm up cycles because I’ve heard chains for the uniform priors on sigma can take a long time to converge. This model still takes a bit of time to chug through the parameter space on my modest laptop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#model_1 &amp;lt;-
#  brm(
#    data = ablation_dta_tbl, family = gaussian,
#    temp ~ 1 + time,
#    prior = c(
#      prior(normal(75, 15), class = Intercept),
#      prior(lognormal(0, .8), class = b),
#      prior(uniform(0, 30), class = sigma)
#    ),
#    iter = 41000, warmup = 40000, chains = 4, cores = 4,
#    seed = 4
#  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-draw-from-the-posterior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Step 5: Draw from the posterior&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The fruits of all my labor! The posterior holds credible combinations for sigma and the slope and intercept (which together describe the mean of the outcome variable we care about). Let’s take a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_samplesM1_tbl &amp;lt;-
  posterior_samples(model_1) %&amp;gt;%
  select(-lp__) %&amp;gt;%
  round(digits = 3)

post_samplesM1_tbl %&amp;gt;%
  head(10) %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;b_Intercept&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;b_time&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sigma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;58.509&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.841&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.682&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;55.983&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.949&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.648&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;56.195&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.937&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.540&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;56.661&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.919&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.474&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;55.143&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.978&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.593&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;55.170&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.977&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;54.908&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.996&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.621&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;58.453&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.836&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.534&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;54.134&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.031&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.647&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;58.713&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.828&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.707&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The plotting function in brms is pretty sweet. I’m not expert in MCMC diagnostics but I do know the “fuzzy caterpillar” look of the trace plots is desirable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-01-creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Posterior_summary() can grab the model results in table form.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_1_summary_tbl &amp;lt;-
  posterior_summary(model_1) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  rownames_to_column() %&amp;gt;%
  as_tibble() %&amp;gt;%
  mutate_if(is.numeric, funs(as.character(signif(., 2)))) %&amp;gt;%
  mutate_at(.vars = c(2:5), funs(as.numeric(.)))

mod_1_summary_tbl %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;rowname&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Est.Error&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Q2.5&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Q97.5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;b_Intercept&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57.00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55.00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;b_time&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.91&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.045&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;sigma&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.60&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.100&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.40&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;lp__&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-790.00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.300&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-790.00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-790.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now let’s see what changes if the time data is centered. Everything is the same here in model_2 except the time_c data which is transformed by subtracting the mean from each value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ablation_dta_tbl &amp;lt;- ablation_dta_tbl %&amp;gt;% mutate(time_c = time - mean(time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#model_2 &amp;lt;-
#  brm(
#    data = ablation_dta_tbl, family = gaussian,
#    temp ~ 1 + time_c,
#    prior = c(
#      prior(normal(75, 15), class = Intercept),
#      prior(lognormal(0, .8), class = b),
#      prior(uniform(0, 30), class = sigma)
#    ),
#    iter = 41000, warmup = 40000, chains = 4, cores = 4,
#    seed = 4
#  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting model_2 to compare with the output of model_1 above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_mod_2_fig &amp;lt;- plot(model_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-01-creating-and-using-a-simple-bayesian-linear-model-in-brms-and-r_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The slope B and sigma are very similar. The intercept is the only difference with model_1 ranging from low to high 50’s. Model 2 is tight around 77. We should visualize the lines proposed by the parameters in the posteriors of our models to understand the uncertainty associated with the mean and also understand why the intercepts are different between models. First, store the posterior samples as a tibble in anticipation for ggplot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_samplesM2_tbl &amp;lt;-
  posterior_samples(model_2) %&amp;gt;%
  select(-lp__) %&amp;gt;%
  round(digits = 3)

post_samplesM2_tbl %&amp;gt;%
  head(10) %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;b_Intercept&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;b_time_c&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sigma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.323&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.894&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.430&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.881&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.516&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.335&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.957&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.571&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.011&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.947&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.776&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.209&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.013&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.691&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.517&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.820&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.488&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.335&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.881&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.682&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.313&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.857&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.538&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.423&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.873&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.569&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;77.302&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.926&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.340&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Visualize the original data (centered and un-centered versions) along with plausible values for regression line of the mean:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_regressionM1_fig &amp;lt;-
  ablation_dta_tbl %&amp;gt;%
  ggplot(aes(x = time, y = temp)) +
  geom_point(
    colour = &amp;quot;#481567FF&amp;quot;,
    size = 2,
    alpha = 0.6
  ) +
  geom_abline(aes(intercept = b_Intercept, slope = b_time),
    data = post_samplesM1_tbl,
    alpha = 0.1, color = &amp;quot;gray50&amp;quot;
  ) +
  geom_abline(
    slope = mean(post_samplesM1_tbl$b_time),
    intercept = mean(post_samplesM1_tbl$b_Intercept),
    color = &amp;quot;blue&amp;quot;, size = 1
  ) +
  labs(
    title = &amp;quot;Regression Line Representing Mean of Slope&amp;quot;,
    subtitle = &amp;quot;Data is As-Observed (No Centering of Predictor)&amp;quot;,
    x = &amp;quot;Time (s)&amp;quot;,
    y = &amp;quot;Temperature (C)&amp;quot;
  )

mean_regressionM2_fig &amp;lt;-
  ablation_dta_tbl %&amp;gt;%
  ggplot(aes(x = time_c, y = temp)) +
  geom_point(
    color = &amp;quot;#55C667FF&amp;quot;,
    size = 2,
    alpha = 0.6
  ) +
  geom_abline(aes(intercept = b_Intercept, slope = b_time_c),
    data = post_samplesM2_tbl,
    alpha = 0.1, color = &amp;quot;gray50&amp;quot;
  ) +
  geom_abline(
    slope = mean(post_samplesM2_tbl$b_time_c),
    intercept = mean(post_samplesM2_tbl$b_Intercept),
    color = &amp;quot;blue&amp;quot;, size = 1
  ) +
  labs(
    title = &amp;quot;Regression Line Representing Mean of Slope&amp;quot;,
    subtitle = &amp;quot;Predictor Data (Time) is Centered&amp;quot;,
    x = &amp;quot;Time (Difference from Mean Time in seconds)&amp;quot;,
    y = &amp;quot;Temperature (C)&amp;quot;
  )


combined_mean_fig &amp;lt;-
  ablation_dta_tbl %&amp;gt;%
  ggplot(aes(x = time, y = temp)) +
  geom_point(
    colour = &amp;quot;#481567FF&amp;quot;,
    size = 2,
    alpha = 0.6
  ) +
  geom_point(
    data = ablation_dta_tbl, aes(x = time_c, y = temp),
    colour = &amp;quot;#55C667FF&amp;quot;,
    size = 2,
    alpha = 0.6
  ) +
  geom_abline(aes(intercept = b_Intercept, slope = b_time),
    data = post_samplesM1_tbl,
    alpha = 0.1, color = &amp;quot;gray50&amp;quot;
  ) +
  geom_abline(
    slope = mean(post_samplesM1_tbl$b_time),
    intercept = mean(post_samplesM1_tbl$b_Intercept),
    color = &amp;quot;blue&amp;quot;, size = 1
  ) +
  geom_abline(aes(intercept = b_Intercept, slope = b_time_c),
    data = post_samplesM2_tbl,
    alpha = 0.1, color = &amp;quot;gray50&amp;quot;
  ) +
  geom_abline(
    slope = mean(post_samplesM2_tbl$b_time_c),
    intercept = mean(post_samplesM2_tbl$b_Intercept),
    color = &amp;quot;blue&amp;quot;, size = 1
  ) +
  labs(
    title = &amp;quot;Regression Line Representing Mean of Slope&amp;quot;,
    subtitle = &amp;quot;Centered and Un-Centered Predictor Data&amp;quot;,
    x = &amp;quot;Time (s)&amp;quot;,
    y = &amp;quot;Temperature (C)&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combined_predicts_fig &amp;lt;- combined_mean_fig + 
  ylim(c(56,90)) +
  labs(title = &amp;quot;Points Represent Observed Data (Green is Centered)&amp;quot;,
       subtitle = &amp;quot;Regression Line Represents Rate of Change of Mean (Grey Bands are Uncertainty)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/combined_predicts_fig.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now everything is clear. The slopes are exactly the same (as we saw in the density plots between model_1 and model_2 in summary()). The intercepts are different because in the centered data (green) the intercept occurs when the predictor equals 0 (its new mean). The outcome variable temp must therefore also be at its mean value in the “knot” of the bow-tie.&lt;/p&gt;
&lt;p&gt;For the un-centered data (purple), the intercept is the value of Temperature when the un-adjusted time is at 0. The range of possible intercepts is much more uncertain here.&lt;/p&gt;
&lt;p&gt;Another way to look at the differences is as a map of the plausible parameter space. We need a plot that can represent 3 parameters: intercept, slope, and sigma. Each point will be a credible combination of the three parameters as observed in 1 row of the posterior distribution tibble(s).&lt;/p&gt;
&lt;p&gt;First, the un-centered model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_spaceM1_fig &amp;lt;- 
  post_samplesM1_tbl[1:1000, ] %&amp;gt;%
  ggplot(aes(x = b_time, y = b_Intercept, color = sigma)) +
  geom_point(alpha = 0.5) +
  geom_density2d(color = &amp;quot;gray30&amp;quot;) +
  scale_color_viridis_c() +
  labs(
    title = &amp;quot;Parameter Space - Model 1 (Un-Centered)&amp;quot;,
    subtitle = &amp;quot;Intercept Represents the Expected Temp at Time = 0&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/p_spaceM1_fig.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now the centered version:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_spaceM2_fig &amp;lt;- 
  post_samplesM2_tbl[1:1000, ] %&amp;gt;%
  ggplot(aes(x = b_time_c, y = b_Intercept, color = sigma)) +
  geom_point(alpha = 0.5) +
  geom_density2d(color = &amp;quot;gray30&amp;quot;) +
  scale_color_viridis_c() +
  labs(
    title = &amp;quot;Parameter Space - Model 2 (Centered)&amp;quot;,
    subtitle = &amp;quot;Intercept Represents the Expected Temp at Mean Time&amp;quot;
  )

#p_spaceM2_fig 
#ggsave(filename = &amp;quot;p_spaceM2_fig.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/p_spaceM2_fig.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These look way different, but part of it is an illusion of the scaling on the y-axis. Remember how the credible values of the intercept were much tighter for the centered model? If we plot them both on the same canvas we can understand better, and it’s pretty (to my eye at least).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_spaceC_tbl &amp;lt;- 
  post_samplesM2_tbl[1:1000, ] %&amp;gt;%
  ggplot(aes(x = b_time_c, y = b_Intercept, color = sigma)) +
  geom_point(alpha = 0.5) +
  geom_point(data = post_samplesM1_tbl, aes(x = b_time, y = b_Intercept, color = sigma), alpha = 0.5) +
  scale_color_viridis_c() +
  labs(
    title = &amp;quot;Credible Parameter Values for Models 1 and 2&amp;quot;,
    subtitle = &amp;quot;Model 1 is Un-Centered, Model 2 is Centered&amp;quot;,
    x = expression(beta[&amp;quot;time&amp;quot;]),
    y = expression(alpha[&amp;quot;Intercept&amp;quot;])) +
  ylim(c(54, 80))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/p_spaceC_tbl.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we see they aren’t as different as they first seemed. They cover very similar ranges for the slope and the un-centered model covers a wider range of plausible intercepts.&lt;/p&gt;
&lt;p&gt;I’ve been looking for a good time to fire up the rayshader package and I’m not throwing away my shot here. Plotting with rayshader feels like a superpower that I shouldn’t be allowed to have. It’s silly how easy it is to make these ridiculous visuals. First, a fancy 3d plot providing some perspective on the relative “heights” of theta.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#par(mfrow = c(1, 1))
#plot_gg(p_spaceC_tbl, width = 5, height = 4, scale = 300, multicore = TRUE, windowsize = c(1200, 960),
#        fov = 70, zoom = 0.45, theta = 330, phi = 40)

#Sys.sleep(0.2)
#render_depth(focus = 0.7, focallength = 200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/3d_params.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want more, this code below renders a video guaranteed to impress small children and executives.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(&amp;quot;av&amp;quot;)
#library(av)

# Set up the camera position and angle
#phivechalf = 30 + 60 * 1/(1 + exp(seq(-7, 20, length.out = 180)/2))
#phivecfull = c(phivechalf, rev(phivechalf))
#thetavec = 0 + 60 * sin(seq(0,359,length.out = 360) * pi/180)
#zoomvec = 0.45 + 0.2 * 1/(1 + exp(seq(-5, 20, length.out = 180)))
#zoomvecfull = c(zoomvec, rev(zoomvec))

# Actually render the video.
#render_movie(filename = &amp;quot;hex_plot_fancy_2&amp;quot;, type = &amp;quot;custom&amp;quot;, 
#            frames = 360,  phi = phivecfull, zoom = zoomvecfull, theta = thetavec)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/hex_plot_fancy_2.gif&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-push-the-parameters-back-through-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Step 6: Push the parameters back through the model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After a lot of work we have finally identified the credible values for our model parameters. We now want to see what sort of predictions our posterior makes. Again, I’ll work with both the centered and un-centered data to try to understand the difference between the approaches. The first step in both cases is to create a sequence of time data to predict off of. For some reason I couldn’t get the predict() function in brms to cooperate so I wrote my own function to predict values. You enter a time value and the function makes a temperature prediction for every combination of mean and standard deviation derived from the parameters in the posterior distribution. Our goal will be to map this function over the sequence of predictor values we just set up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#sequence of time data to predict off of.  Could use the same for both models but I created 2 for clarity
time_seq_tbl   &amp;lt;- tibble(pred_time   = seq(from = -15, to = 60, by = 1))
time_seq_tbl_2 &amp;lt;- tibble(pred_time_2 = seq(from = -15, to = 60, by = 1))

#function that takes a time value and makes a prediction using model_1 (un-centered) 
rk_predict &amp;lt;- 
function(time_to_sim){
  rnorm(n = nrow(post_samplesM1_tbl),
        mean = post_samplesM1_tbl$b_Intercept + post_samplesM1_tbl$b_time*time_to_sim,
        sd = post_samplesM1_tbl$sigma
  )
}

#function that takes a time value and makes a prediction using model_2 (centered)
rk_predict2 &amp;lt;- 
function(time_to_sim){
  rnorm(n = nrow(post_samplesM2_tbl),
        mean = post_samplesM2_tbl$b_Intercept + post_samplesM2_tbl$b_time_c*time_to_sim,
        sd = post_samplesM2_tbl$sigma
  )
}

#map the first prediction function over all values in the time sequence
#then calculate the .025 and .975 quantiles in anticipation of 95% prediction intervals
predicts_m1_tbl &amp;lt;- time_seq_tbl %&amp;gt;%
  mutate(preds_for_this_time = map(pred_time, rk_predict)) %&amp;gt;%
  mutate(percentile_2.5  = map_dbl(preds_for_this_time, ~quantile(., .025))) %&amp;gt;%
  mutate(percentile_97.5 = map_dbl(preds_for_this_time, ~quantile(., .975)))
    
#same for the 2nd prediction function
predicts_m2_tbl &amp;lt;- time_seq_tbl_2 %&amp;gt;%
  mutate(preds_for_this_time = map(pred_time_2, rk_predict2)) %&amp;gt;%
  mutate(percentile_2.5  = map_dbl(preds_for_this_time, ~quantile(., .025))) %&amp;gt;%
  mutate(percentile_97.5 = map_dbl(preds_for_this_time, ~quantile(., .975)))   

#visualize what is stored in the nested prediction cells (sanity check)
test_array &amp;lt;- predicts_m2_tbl[1, 2] %&amp;gt;% unnest(cols = c(preds_for_this_time))
test_array %&amp;gt;% 
  round(digits = 2) %&amp;gt;%
  head(5) %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;preds_for_this_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;68.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;61.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;65.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;62.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;64.05&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And now the grand finale - overlay the 95% prediction intervals on the original data along with the credible values of mean. We see there is no difference between the predictions made from centered data vs. un-centered.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;big_enchilada &amp;lt;- 
  tibble(h=0) %&amp;gt;%
  ggplot() +
  geom_point(
    data = ablation_dta_tbl, aes(x = time, y = temp),
    colour = &amp;quot;#481567FF&amp;quot;,
    size = 2,
    alpha = 0.6
  ) +
  geom_point(
    data = ablation_dta_tbl, aes(x = time_c, y = temp),
    colour = &amp;quot;#55C667FF&amp;quot;,
    size = 2,
    alpha = 0.6
  ) +
  geom_abline(aes(intercept = b_Intercept, slope = b_time),
    data = post_samplesM1_tbl,
    alpha = 0.1, color = &amp;quot;gray50&amp;quot;
  ) +
  geom_abline(
    slope = mean(post_samplesM1_tbl$b_time),
    intercept = mean(post_samplesM1_tbl$b_Intercept),
    color = &amp;quot;blue&amp;quot;, size = 1
  ) +
  geom_abline(aes(intercept = b_Intercept, slope = b_time_c),
    data = post_samplesM2_tbl,
    alpha = 0.1, color = &amp;quot;gray50&amp;quot;
  ) +
  geom_abline(
    slope = mean(post_samplesM2_tbl$b_time_c),
    intercept = mean(post_samplesM2_tbl$b_Intercept),
    color = &amp;quot;blue&amp;quot;, size = 1
  ) +
  geom_ribbon(
  data = predicts_m1_tbl, aes(x = predicts_m1_tbl$pred_time, ymin = predicts_m1_tbl$percentile_2.5, ymax = predicts_m1_tbl$percentile_97.5), alpha = 0.25, fill = &amp;quot;pink&amp;quot;, color = &amp;quot;black&amp;quot;, size = .3
) +
  geom_ribbon(
  data = predicts_m2_tbl, aes(x = predicts_m2_tbl$pred_time_2, ymin = predicts_m2_tbl$percentile_2.5, ymax = predicts_m2_tbl$percentile_97.5), alpha = 0.4, fill = &amp;quot;pink&amp;quot;, color = &amp;quot;black&amp;quot;, size = .3
) +
  labs(
    title = &amp;quot;Regression Line Representing Mean of Slope&amp;quot;,
    subtitle = &amp;quot;Centered and Un-Centered Predictor Data&amp;quot;,
    x = &amp;quot;Time (s)&amp;quot;,
    y = &amp;quot;Temperature (C)&amp;quot;
  ) +
  scale_x_continuous(limits = c(-10, 37), expand = c(0, 0)) +
  scale_y_continuous(limits = c(40, 120), expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/./img/big_enchilada.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What a ride! This seemingly simple problem really stretched my brain. There are still a lot of question I want to go deeper on - diagnostics for the MCMC, impact of the regularizing priors, different between this workflow and frequentist at various sample sizes and priors, etc… but that will have to wait for another day.&lt;/p&gt;
&lt;p&gt;Thank you for reading.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Statistical Rethinking, &lt;a href=&#34;https://github.com/rmcelreath/statrethinking_winter2019&#34; class=&#34;uri&#34;&gt;https://github.com/rmcelreath/statrethinking_winter2019&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1547527116001806&#34; class=&#34;uri&#34;&gt;https://www.sciencedirect.com/science/article/abs/pii/S1547527116001806&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;There’s a funky bug in ggExtra which makes you break this code into 2 chunks when working in Markdown, &lt;a href=&#34;https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html&lt;/a&gt;&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Heart Disease Prediction From Patient Data in R</title>
      <link>/post/heart-disease-prediction-from-patient-data-in-r/</link>
      <pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/heart-disease-prediction-from-patient-data-in-r/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post I’ll be attempting to leverage the parsnip package in R to run through some straightforward predictive analytics/machine learning. Parsnip provides a flexible and consistent interface to apply common regression and classification algorithms in R. I’ll be working with the Cleveland Clinic Heart Disease dataset which contains 13 variables related to patient diagnostics and one outcome variable indicating the presence or absence of heart disease.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The data was accessed from the UCI Machine Learning Repository in September 2019.&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. The goal is to be able to accurately classify as having or not having heart disease based on diagnostic test data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./img/cda.jpg&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Load the libraries to be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Load libraries
library(tidyverse)
library(kableExtra)
library(rsample)
library(recipes)
library(parsnip)
library(yardstick)
library(viridisLite)
library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first part of the analysis is to read in the data set and clean the column names up a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Read in data
heart_disease_dataset &amp;lt;- read.csv(file = &amp;quot;processed.cleveland.data&amp;quot;, header = F)

#Prepare column names
names &amp;lt;- c(&amp;quot;Age&amp;quot;,
           &amp;quot;Sex&amp;quot;,
           &amp;quot;Chest_Pain_Type&amp;quot;,
           &amp;quot;Resting_Blood_Pressure&amp;quot;,
           &amp;quot;Serum_Cholesterol&amp;quot;,
           &amp;quot;Fasting_Blood_Sugar&amp;quot;,
           &amp;quot;Resting_ECG&amp;quot;,
           &amp;quot;Max_Heart_Rate_Achieved&amp;quot;,
           &amp;quot;Exercise_Induced_Angina&amp;quot;,
           &amp;quot;ST_Depression_Exercise&amp;quot;,
           &amp;quot;Peak_Exercise_ST_Segment&amp;quot;,
           &amp;quot;Num_Major_Vessels_Flouro&amp;quot;,
           &amp;quot;Thalassemia&amp;quot;,
           &amp;quot;Diagnosis_Heart_Disease&amp;quot;)

#Apply column names to the dataframe
colnames(heart_disease_dataset) &amp;lt;- names

#Glimpse data to verify new column names are in place
heart_disease_dataset %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 303
## Variables: 14
## $ Age                      &amp;lt;dbl&amp;gt; 63, 67, 67, 37, 41, 56, 62, 57, 63, 5...
## $ Sex                      &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1...
## $ Chest_Pain_Type          &amp;lt;dbl&amp;gt; 1, 4, 4, 3, 2, 2, 4, 4, 4, 4, 4, 2, 3...
## $ Resting_Blood_Pressure   &amp;lt;dbl&amp;gt; 145, 160, 120, 130, 130, 120, 140, 12...
## $ Serum_Cholesterol        &amp;lt;dbl&amp;gt; 233, 286, 229, 250, 204, 236, 268, 35...
## $ Fasting_Blood_Sugar      &amp;lt;dbl&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1...
## $ Resting_ECG              &amp;lt;dbl&amp;gt; 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2...
## $ Max_Heart_Rate_Achieved  &amp;lt;dbl&amp;gt; 150, 108, 129, 187, 172, 178, 160, 16...
## $ Exercise_Induced_Angina  &amp;lt;dbl&amp;gt; 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1...
## $ ST_Depression_Exercise   &amp;lt;dbl&amp;gt; 2.3, 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0....
## $ Peak_Exercise_ST_Segment &amp;lt;dbl&amp;gt; 3, 2, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 2...
## $ Num_Major_Vessels_Flouro &amp;lt;fct&amp;gt; 0.0, 3.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0....
## $ Thalassemia              &amp;lt;fct&amp;gt; 6.0, 3.0, 7.0, 3.0, 3.0, 3.0, 3.0, 3....
## $ Diagnosis_Heart_Disease  &amp;lt;int&amp;gt; 0, 2, 1, 0, 0, 0, 3, 0, 2, 1, 0, 0, 2...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 14 variables provided in the data set and the last one is the dependent variable that we want to be able to predict. Here is a summary of what the other variables mean:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Age&lt;/strong&gt;: Age of subject&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sex&lt;/strong&gt;: Gender of subject:&lt;br /&gt;
0 = female 1 = male&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chest-pain type&lt;/strong&gt;: Type of chest-pain experienced by the individual:&lt;br /&gt;
1 = typical angina&lt;br /&gt;
2 = atypical angina&lt;br /&gt;
3 = non-angina pain&lt;br /&gt;
4 = asymptomatic angina&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Resting Blood Pressure&lt;/strong&gt;: Resting blood pressure in mm Hg&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Serum Cholesterol&lt;/strong&gt;: Serum cholesterol in mg/dl&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fasting Blood Sugar&lt;/strong&gt;: Fasting blood sugar level relative to 120 mg/dl: 0 = fasting blood sugar &amp;lt;= 120 mg/dl&lt;br /&gt;
1 = fasting blood sugar &amp;gt; 120 mg/dl&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Resting ECG&lt;/strong&gt;: Resting electrocardiographic results&lt;br /&gt;
0 = normal&lt;br /&gt;
1 = ST-T wave abnormality&lt;br /&gt;
2 = left ventricle hyperthrophy&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Max Heart Rate Achieved&lt;/strong&gt;: Max heart rate of subject&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Exercise Induced Angina&lt;/strong&gt;:&lt;br /&gt;
0 = no 1 = yes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ST Depression Induced by Exercise Relative to Rest&lt;/strong&gt;: ST Depression of subject&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Peak Exercise ST Segment&lt;/strong&gt;:&lt;br /&gt;
1 = Up-sloaping&lt;br /&gt;
2 = Flat&lt;br /&gt;
3 = Down-sloaping&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Number of Major Vessels (0-3) Visible on Flouroscopy&lt;/strong&gt;: Number of visible vessels under flouro&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Thal&lt;/strong&gt;: Form of thalassemia: &lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;br /&gt;
3 = normal&lt;br /&gt;
6 = fixed defect&lt;br /&gt;
7 = reversible defect&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Diagnosis of Heart Disease&lt;/strong&gt;: Indicates whether subject is suffering from heart disease or not:&lt;br /&gt;
0 = absence&lt;br /&gt;
1, 2, 3, 4 = heart disease present&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A closer look at the data identifies some NA and “?” values that will need to be addressed in the cleaning step. We also want to know the number of observations in the dependent variable column to understand if the dataset is relatively balanced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Determine the number of values in each level of dependent variable
heart_disease_dataset %&amp;gt;% 
  drop_na() %&amp;gt;%
  group_by(Diagnosis_Heart_Disease) %&amp;gt;%
  count() %&amp;gt;% 
  ungroup() %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 2)) %&amp;gt;% kable_styling(&amp;quot;full_width&amp;quot; = F)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Diagnosis_Heart_Disease
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
164
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
55
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
36
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
35
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Identify the different levels of Thalassemia
heart_disease_dataset %&amp;gt;% 
  drop_na() %&amp;gt;%
  group_by(Thalassemia) %&amp;gt;%
  count() %&amp;gt;% 
  ungroup() %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 2)) %&amp;gt;% kable_styling(&amp;quot;full_width&amp;quot; = F)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Thalassemia
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
?
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
166
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
117
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Since any value above 0 in ‘Diagnosis_Heart_Disease’ (column 14) indicates the presence of heart disease, we can lump all levels &amp;gt; 0 together so the classification predictions are binary - Yes or No (1 or 0). The total count of positive heart disease results is less than the number of negative results so the fct_lump() call with default arguments will convert that variable from 4 levels to 2.&lt;/p&gt;
&lt;p&gt;The data cleaning pipeline below deals with NA values, converts some variables to factors, lumps the dependent variable into two buckets, removes the rows that had “?” for observations, and reorders the variables within the dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Drop NA&amp;#39;s, convert to factors, lump target variable to 2 levels, remove &amp;quot;?&amp;quot;, reorder variables
heart_dataset_clean_tbl &amp;lt;- heart_disease_dataset %&amp;gt;% 
    drop_na() %&amp;gt;%
    mutate_at(c(&amp;quot;Resting_ECG&amp;quot;, 
                &amp;quot;Fasting_Blood_Sugar&amp;quot;, 
                &amp;quot;Sex&amp;quot;, 
                &amp;quot;Diagnosis_Heart_Disease&amp;quot;, 
                &amp;quot;Exercise_Induced_Angina&amp;quot;,
                &amp;quot;Peak_Exercise_ST_Segment&amp;quot;, 
                &amp;quot;Chest_Pain_Type&amp;quot;), as_factor) %&amp;gt;%
    mutate(Num_Major_Vessels_Flouro = as.numeric(Num_Major_Vessels_Flouro)) %&amp;gt;%
    mutate(Diagnosis_Heart_Disease = fct_lump(Diagnosis_Heart_Disease, other_level = &amp;quot;1&amp;quot;)) %&amp;gt;% 
    filter(Thalassemia != &amp;quot;?&amp;quot;) %&amp;gt;%
    select(Age, 
           Resting_Blood_Pressure, 
           Serum_Cholesterol, 
           Max_Heart_Rate_Achieved, 
           ST_Depression_Exercise,
           Num_Major_Vessels_Flouro,
           everything())

#Glimpse data
heart_dataset_clean_tbl %&amp;gt;%
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 301
## Variables: 14
## $ Age                      &amp;lt;dbl&amp;gt; 63, 67, 67, 37, 41, 56, 62, 57, 63, 5...
## $ Resting_Blood_Pressure   &amp;lt;dbl&amp;gt; 145, 160, 120, 130, 130, 120, 140, 12...
## $ Serum_Cholesterol        &amp;lt;dbl&amp;gt; 233, 286, 229, 250, 204, 236, 268, 35...
## $ Max_Heart_Rate_Achieved  &amp;lt;dbl&amp;gt; 150, 108, 129, 187, 172, 178, 160, 16...
## $ ST_Depression_Exercise   &amp;lt;dbl&amp;gt; 2.3, 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0....
## $ Num_Major_Vessels_Flouro &amp;lt;dbl&amp;gt; 2, 5, 4, 2, 2, 2, 4, 2, 3, 2, 2, 2, 3...
## $ Sex                      &amp;lt;fct&amp;gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1...
## $ Chest_Pain_Type          &amp;lt;fct&amp;gt; 1, 4, 4, 3, 2, 2, 4, 4, 4, 4, 4, 2, 3...
## $ Fasting_Blood_Sugar      &amp;lt;fct&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1...
## $ Resting_ECG              &amp;lt;fct&amp;gt; 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2...
## $ Exercise_Induced_Angina  &amp;lt;fct&amp;gt; 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1...
## $ Peak_Exercise_ST_Segment &amp;lt;fct&amp;gt; 3, 2, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 2...
## $ Thalassemia              &amp;lt;fct&amp;gt; 6.0, 3.0, 7.0, 3.0, 3.0, 3.0, 3.0, 3....
## $ Diagnosis_Heart_Disease  &amp;lt;fct&amp;gt; 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time for some basic exploratory data analysis. The workflow below breaks out the categorical variables and visualizes them on a faceted bar plot. I’m recoding the factors levels from numeric back to text-based so the labels are easy to interpret on the plots and stripping the y-axis labels since the relative differences are what matters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Select categorical vars, recode them to their character values, convert to long format
hd_long_fact_tbl &amp;lt;- heart_dataset_clean_tbl  %&amp;gt;%
  select(Sex,
         Chest_Pain_Type,
         Fasting_Blood_Sugar,
         Resting_ECG,
         Exercise_Induced_Angina,
         Peak_Exercise_ST_Segment,
         Thalassemia,
         Diagnosis_Heart_Disease) %&amp;gt;%
  mutate(Sex = recode_factor(Sex, `0` = &amp;quot;female&amp;quot;, 
                                  `1` = &amp;quot;male&amp;quot; ),
         Chest_Pain_Type = recode_factor(Chest_Pain_Type, `1` = &amp;quot;typical&amp;quot;,   
                                                          `2` = &amp;quot;atypical&amp;quot;,
                                                          `3` = &amp;quot;non-angina&amp;quot;, 
                                                          `4` = &amp;quot;asymptomatic&amp;quot;),
         Fasting_Blood_Sugar = recode_factor(Fasting_Blood_Sugar, `0` = &amp;quot;&amp;lt;= 120 mg/dl&amp;quot;, 
                                                                  `1` = &amp;quot;&amp;gt; 120 mg/dl&amp;quot;),
         Resting_ECG = recode_factor(Resting_ECG, `0` = &amp;quot;normal&amp;quot;,
                                                  `1` = &amp;quot;ST-T abnormality&amp;quot;,
                                                  `2` = &amp;quot;LV hypertrophy&amp;quot;),
         Exercise_Induced_Angina = recode_factor(Exercise_Induced_Angina, `0` = &amp;quot;no&amp;quot;,
                                                                          `1` = &amp;quot;yes&amp;quot;),
         Peak_Exercise_ST_Segment = recode_factor(Peak_Exercise_ST_Segment, `1` = &amp;quot;up-sloaping&amp;quot;,
                                                                            `2` = &amp;quot;flat&amp;quot;,
                                                                            `3` = &amp;quot;down-sloaping&amp;quot;),
         Thalassemia = recode_factor(Thalassemia, `3` = &amp;quot;normal&amp;quot;,
                                                  `6` = &amp;quot;fixed defect&amp;quot;,
                                                  `7` = &amp;quot;reversible defect&amp;quot;)) %&amp;gt;%
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;value&amp;quot;, -Diagnosis_Heart_Disease)

#Visualize with bar plot
hd_long_fact_tbl %&amp;gt;% 
  ggplot(aes(value)) +
    geom_bar(aes(x        = value, 
                 fill     = Diagnosis_Heart_Disease), 
                 alpha    = .6, 
                 position = &amp;quot;dodge&amp;quot;, 
                 color    = &amp;quot;black&amp;quot;,
                 width    = .8
             ) +
    labs(x = &amp;quot;&amp;quot;,
         y = &amp;quot;&amp;quot;,
         title = &amp;quot;Scaled Effect of Categorical Variables&amp;quot;) +
    theme(
         axis.text.y  = element_blank(),
         axis.ticks.y = element_blank()) +
    facet_wrap(~ key, scales = &amp;quot;free&amp;quot;, nrow = 4) +
    scale_fill_manual(
         values = c(&amp;quot;#fde725ff&amp;quot;, &amp;quot;#20a486ff&amp;quot;),
         name   = &amp;quot;Heart\nDisease&amp;quot;,
         labels = c(&amp;quot;No HD&amp;quot;, &amp;quot;Yes HD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-29-heart-disease-prediction-from-patient-data-in-r_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I prefer boxplots for evaluating the numeric variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Must gather() data first in order to facet wrap by key 
#(default gather call puts all var names into new key col)
hd_long_cont_tbl &amp;lt;- heart_dataset_clean_tbl  %&amp;gt;%
  select(Age,
         Resting_Blood_Pressure,
         Serum_Cholesterol,
         Max_Heart_Rate_Achieved,
         ST_Depression_Exercise,
         Num_Major_Vessels_Flouro,
         Diagnosis_Heart_Disease) %&amp;gt;% 
  gather(key   = &amp;quot;key&amp;quot;, 
         value = &amp;quot;value&amp;quot;,
         -Diagnosis_Heart_Disease)

#Visualize numeric variables as boxplots
hd_long_cont_tbl %&amp;gt;% 
  ggplot(aes(y = value)) +
       geom_boxplot(aes(fill = Diagnosis_Heart_Disease),
                      alpha  = .6,
                      fatten = .7) +
        labs(x = &amp;quot;&amp;quot;,
             y = &amp;quot;&amp;quot;,
             title = &amp;quot;Boxplots for Numeric Variables&amp;quot;) +
      scale_fill_manual(
            values = c(&amp;quot;#fde725ff&amp;quot;, &amp;quot;#20a486ff&amp;quot;),
            name   = &amp;quot;Heart\nDisease&amp;quot;,
            labels = c(&amp;quot;No HD&amp;quot;, &amp;quot;Yes HD&amp;quot;)) +
      theme(
         axis.text.x  = element_blank(),
         axis.ticks.x = element_blank()) +
      facet_wrap(~ key, 
                 scales = &amp;quot;free&amp;quot;, 
                 ncol   = 2) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-29-heart-disease-prediction-from-patient-data-in-r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt; The faceted plots for categorical and numeric variables suggest the following conditions are associated with increased prevalence of heart disease (note: this does not mean the relationship is causal).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asymptomatic angina chest pain (relative to typical angina chest pain, atypical angina pain, or non-angina pain)&lt;/li&gt;
&lt;li&gt;Presence of exercise induced angina&lt;/li&gt;
&lt;li&gt;Lower fasting blood sugar&lt;/li&gt;
&lt;li&gt;Flat or down-sloaping peak exercise ST segment&lt;/li&gt;
&lt;li&gt;Presence of left ventricle hypertrophy&lt;/li&gt;
&lt;li&gt;Male&lt;/li&gt;
&lt;li&gt;Higher thelassemia score&lt;/li&gt;
&lt;li&gt;Higher age&lt;/li&gt;
&lt;li&gt;Lower max heart rate achieved&lt;/li&gt;
&lt;li&gt;Higher resting blood pressure&lt;/li&gt;
&lt;li&gt;Higher cholesterol&lt;/li&gt;
&lt;li&gt;Higher ST depression induced by exercise relative to rest&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can’t all be cardiologists but these do seem to pass the eye check. Particularly: age, blood pressure, cholesterol, and sex all point in the right direction based on what we generally know about the world around us. This provides a nice phase gate to let us proceed with the analysis.&lt;/p&gt;
&lt;p&gt;Highly correlated variables can lead to overly complicated models or wonky predictions. The ggcorr() function from GGally package provides a nice, clean correlation matrix of the numeric variables. The default method is Pearson which I use here first. Pearson isn’t ideal if the data is skewed or has a lot of outliers so I’ll check using the rank-based Kendall method as well.&lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlation matrix using Pearson method, default method is Pearson
heart_dataset_clean_tbl %&amp;gt;% ggcorr(high       = &amp;quot;#20a486ff&amp;quot;,
                                   low        = &amp;quot;#fde725ff&amp;quot;,
                                   label      = TRUE, 
                                   hjust      = .75, 
                                   size       = 3, 
                                   label_size = 3,
                                   nbreaks    = 5
                                              ) +
  labs(title = &amp;quot;Correlation Matrix&amp;quot;,
  subtitle = &amp;quot;Pearson Method Using Pairwise Obervations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-29-heart-disease-prediction-from-patient-data-in-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlation matrix using Kendall method
heart_dataset_clean_tbl %&amp;gt;% ggcorr(method     = c(&amp;quot;pairwise&amp;quot;, &amp;quot;kendall&amp;quot;),
                                   high       = &amp;quot;#20a486ff&amp;quot;,
                                   low        = &amp;quot;#fde725ff&amp;quot;,
                                   label      = TRUE, 
                                   hjust      = .75, 
                                   size       = 3, 
                                   label_size = 3,
                                   nbreaks    = 5
                                   ) +
  labs(title = &amp;quot;Correlation Matrix&amp;quot;,
  subtitle = &amp;quot;Kendall Method Using Pairwise Observations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-29-heart-disease-prediction-from-patient-data-in-r_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are very minor differences between the Pearson and Kendall results. No variables appear to be highly correlated. As such, it seems reasonable to stay with the original 14 variables as we proceed into the modeling section.&lt;/p&gt;
&lt;p&gt;The plan is to split up the original data set to form a training group and testing group. The training group will be used to fit the model while the testing group will be used to evaluate predictions. The initial_split() function creates a split object which is just an efficient way to store both the training and testing sets. The training() and testing() functions are used to extract the appropriate dataframes out of the split object when needed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set seed for repeatability
set.seed(1333)

#create split object 
train_test_split &amp;lt;- heart_dataset_clean_tbl %&amp;gt;% initial_split(prop = .8, strata = &amp;quot;Diagnosis_Heart_Disease&amp;quot;)

#pipe split obj to training() fcn to create training set
train_tbl &amp;lt;- train_test_split %&amp;gt;% training()

#pipe split obj to testing() fcn to create test set
test_tbl &amp;lt;- train_test_split %&amp;gt;% testing()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We chose to do our data preparation early on during the cleaning phase. For more complicated modeling operations it may be desirable to set up a recipe to do the pre-processing in a repeatable and reversible fashion and I chose here to leave some placeholder lines commented out and available for future work. The recipe is the spot to transform, scale, or binarize the data. We have to tell the recipe() function what we want to model: Diagnosis_Heart_Disease as a function of all the other variables (not needed here since we took care of the necessary conversions). The training data should be used exclusively to train the recipe to avoid data leakage. After giving the model syntax to the recipe, the data is piped into the prep() function which will extract all the processing parameters (if we had implemented processing steps here). The trained recipe is stored as an object and bake function is used to apply the trained recipe to a new (test) data set.&lt;/p&gt;
&lt;p&gt;Juice() is a shortcut to extract the finalized training set which is already embedded in the recipe by default. Calling the bake() function and providing the recipe and a new data set will apply the processing steps to that dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Set up recipe (use training data here to avoid leakage)
the_recipe &amp;lt;- recipe(Diagnosis_Heart_Disease ~ . , data = train_tbl) %&amp;gt;%
              #[Processing Step 1]
              #[Processing Step 2]
              prep(train_tbl, retain = TRUE)

#Apply recipe to training data to create processed training_data_obj (already populated in the recipe object)
train_processed_data &amp;lt;- juice(the_recipe)

#Apply recipe to test data to create processed test_data_obj
test_processed_data &amp;lt;- bake(the_recipe, new_data = test_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the training and testing data have been processed and stored, the logistic regression model can be set up using the parsnip workflow. Parsnip uses a 3-step process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;specify the model and its arguments&lt;/li&gt;
&lt;li&gt;set the engine (how the model is created)&lt;/li&gt;
&lt;li&gt;fit the model to the processed training data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logistic regression is a convenient first model to work with since it is relatively easy to implement and yields results that have intuitive meaning. It can be easily interpreted when the odds ratio is calculated from the model structure. &lt;a href=&#34;#fn5&#34; class=&#34;footnoteRef&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Set up and train the model using processed training_data_obj
set.seed(100)
log_regr_hd_model &amp;lt;- logistic_reg(mode = &amp;quot;classification&amp;quot;) %&amp;gt;%
                     set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;% 
                     fit(Diagnosis_Heart_Disease ~ ., data = train_processed_data)

#Take a look at model coefficients and add odds ratio for interpretability
broom::tidy(log_regr_hd_model$fit) %&amp;gt;%
  arrange(desc(estimate)) %&amp;gt;% 
  mutate(odds_ratio = exp(estimate)) %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 5), digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
term
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
std.error
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
statistic
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
p.value
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
odds_ratio
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Thalassemia7.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.932
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.519
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.726
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.906
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Chest_Pain_Type4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.694
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.770
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.201
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.028
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.443
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Sex1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.473
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.648
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.274
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.023
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.364
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Num_Major_Vessels_Flouro
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.264
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.307
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.119
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.538
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Chest_Pain_Type2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.255
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.874
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.436
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.151
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.507
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Resting_ECG2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.022
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.447
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.287
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.022
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.780
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Peak_Exercise_ST_Segment3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.003
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.984
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.020
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.308
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.727
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Peak_Exercise_ST_Segment2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.833
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.551
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.512
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.130
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.300
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Exercise_Induced_Angina1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.704
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.526
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.339
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.181
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.023
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Resting_ECG1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.675
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.314
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.204
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.839
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.965
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ST_Depression_Exercise
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.340
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.268
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.267
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.205
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.405
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Thalassemia6.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.127
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.882
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.144
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.885
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.136
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Resting_Blood_Pressure
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.036
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.014
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.535
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.011
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.037
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Serum_Cholesterol
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.003
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.005
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.644
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.520
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.003
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Max_Heart_Rate_Achieved
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.032
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.014
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-2.271
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.023
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.969
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Age
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.035
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.029
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1.198
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.231
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.966
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Chest_Pain_Type3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.282
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.770
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.366
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.714
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.755
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fasting_Blood_Sugar1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.684
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.718
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.953
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.341
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.504
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-6.350
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.439
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1.846
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.065
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.002
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the above code I’ve converted the estimate of the coefficient into the odds ratio. The odds ratio represents the odds that an outcome will occur given the presence of a specific predictor, compared to the odds of the outcome occurring in the absence of that predictor, assuming all other predictors remain constant. The odds ratio is calculated from the exponential function of the coefficient estimate based on a unit increase in the predictor. An example with a numeric variable: for 1 mm Hg increased in resting blood pressure rest_bp, the odds of having heart disease increases by a factor of 1.04.&lt;/p&gt;
&lt;p&gt;Now let’s feed the model the testing data that we held out from the fitting process. It’s the first time the model will have seen these data so we should get a fair assessment (absent of over-fitting). The new_data argument in the predict() function is used to supply the test data to the model and have it output a vector of predictions, one for each observation in the testing data. The results vector can be added as a column into the original dataframe to append the predictions next to the true values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Make predictions using testing set
first_training_prediction &amp;lt;- predict(log_regr_hd_model, 
                                     new_data = test_tbl, 
                                     type     = &amp;quot;class&amp;quot;)

#Add predictions as new column in heart data set
first_training_prediction_full_tbl &amp;lt;- test_processed_data %&amp;gt;% 
  mutate(Predicted_Heart_Disease = first_training_prediction$.pred_class)

#Glimpse data
first_training_prediction_full_tbl %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 59
## Variables: 15
## $ Age                      &amp;lt;dbl&amp;gt; 56, 63, 56, 52, 54, 60, 64, 43, 65, 4...
## $ Resting_Blood_Pressure   &amp;lt;dbl&amp;gt; 120, 130, 140, 172, 140, 117, 140, 12...
## $ Serum_Cholesterol        &amp;lt;dbl&amp;gt; 236, 254, 294, 199, 239, 230, 335, 17...
## $ Max_Heart_Rate_Achieved  &amp;lt;dbl&amp;gt; 178, 147, 153, 162, 160, 160, 158, 12...
## $ ST_Depression_Exercise   &amp;lt;dbl&amp;gt; 0.8, 1.4, 1.3, 0.5, 1.2, 1.4, 0.0, 2....
## $ Num_Major_Vessels_Flouro &amp;lt;dbl&amp;gt; 2, 3, 2, 2, 2, 4, 2, 2, 5, 2, 2, 2, 2...
## $ Sex                      &amp;lt;fct&amp;gt; 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1...
## $ Chest_Pain_Type          &amp;lt;fct&amp;gt; 2, 4, 2, 3, 4, 4, 3, 4, 4, 1, 4, 3, 3...
## $ Fasting_Blood_Sugar      &amp;lt;fct&amp;gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1...
## $ Resting_ECG              &amp;lt;fct&amp;gt; 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 2...
## $ Exercise_Induced_Angina  &amp;lt;fct&amp;gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0...
## $ Peak_Exercise_ST_Segment &amp;lt;fct&amp;gt; 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3...
## $ Thalassemia              &amp;lt;fct&amp;gt; 3.0, 7.0, 3.0, 7.0, 3.0, 7.0, 3.0, 7....
## $ Diagnosis_Heart_Disease  &amp;lt;fct&amp;gt; 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0...
## $ Predicted_Heart_Disease  &amp;lt;fct&amp;gt; 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A confusion matrix is a visual way to display the results of the model’s predictions. It’s not just the ability to predict the presence of heart disease that is of interest - we also want to know the number of times the model successfully predicts the absence of heart disease. Likewise, we want to know the number of false positives and false negatives. The confusion matrix captures all these metrics nicely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Use predictions col and truth col to make a confusion matrix object
conf_mat_obj &amp;lt;- first_training_prediction_full_tbl %&amp;gt;% 
  conf_mat(truth    = Diagnosis_Heart_Disease, 
           estimate = Predicted_Heart_Disease)

#Call conf_mat and supply columns for truth, prediction
#Pluck() to extract the conf_matrix data into cols and convert to tibble for plotting
conf_matrix_plt_obj &amp;lt;- first_training_prediction_full_tbl %&amp;gt;% 
  conf_mat(truth    = Diagnosis_Heart_Disease, 
           estimate = Predicted_Heart_Disease) %&amp;gt;%
  pluck(1) %&amp;gt;%
  as_tibble() %&amp;gt;%
  mutate(&amp;quot;outcome&amp;quot; = c(&amp;quot;true_negative&amp;quot;,
                       &amp;quot;false_positive&amp;quot;,
                       &amp;quot;false_negative&amp;quot;,
                       &amp;quot;true_positive&amp;quot;)) %&amp;gt;%
  mutate(Prediction = recode(Prediction, &amp;quot;0&amp;quot; = &amp;quot;No Heart Disease&amp;quot;,
                                         &amp;quot;1&amp;quot; = &amp;quot;Heart Disease&amp;quot;)) %&amp;gt;%
  mutate(Truth = recode(Truth,  &amp;quot;0&amp;quot; = &amp;quot;No Heart Disease&amp;quot;,
                                &amp;quot;1&amp;quot; = &amp;quot;Heart Disease&amp;quot;))

#Convert to kable format
conf_matrix_plt_obj %&amp;gt;% kable(align = rep(&amp;quot;c&amp;quot;, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Prediction
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Truth
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
outcome
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
No Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
No Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
true_negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
No Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
false_positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
No Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
false_negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Heart Disease
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
true_positive
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Plot confusion matrix
p1 &amp;lt;- conf_matrix_plt_obj %&amp;gt;% ggplot(aes(x = Truth, y = Prediction)) +
  geom_tile(aes(fill = n), alpha = .8) +
  geom_text(aes(label = n), color = &amp;quot;white&amp;quot;) +
  scale_fill_viridis_c() +
  theme(legend.title = element_blank()) +
  labs(
    title    = &amp;quot;Confusion Matrix&amp;quot;,
    subtitle = &amp;quot;Heart Disease Prediction Using Logistic Regression&amp;quot;
  )
  
p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-29-heart-disease-prediction-from-patient-data-in-r_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Calling summary() on the confusion_matrix_obj gives all the performance measures
#Filter to the ones we care about
log_reg_performance_tbl &amp;lt;- summary(conf_mat_obj) %&amp;gt;% filter(
                                 .metric == &amp;quot;accuracy&amp;quot; | 
                                 .metric == &amp;quot;sens&amp;quot; |
                                 .metric == &amp;quot;spec&amp;quot; |
                                 .metric == &amp;quot;ppv&amp;quot;  |
                                 .metric == &amp;quot;npv&amp;quot;  |
                                 .metric == &amp;quot;f_meas&amp;quot;) %&amp;gt;%
  select(-.estimator) %&amp;gt;%
  rename(&amp;quot;metric&amp;quot; = .metric, 
         &amp;quot;estimate&amp;quot; = .estimate) %&amp;gt;%
  mutate(&amp;quot;estimate&amp;quot; = estimate %&amp;gt;% signif(digits = 3)) %&amp;gt;%
  mutate(metric = recode(metric, &amp;quot;sens&amp;quot; = &amp;quot;sensitivity&amp;quot;),
         metric = recode(metric, &amp;quot;spec&amp;quot; = &amp;quot;specificity&amp;quot;),
         metric = recode(metric, &amp;quot;ppv&amp;quot;  = &amp;quot;positive predictive value&amp;quot;),
         metric = recode(metric, &amp;quot;npv&amp;quot;  = &amp;quot;negative predictive value&amp;quot;)) %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 3))
  
#Display perfomance summary as kable
log_reg_performance_tbl &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
metric
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
estimate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.847
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
sensitivity
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.906
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
specificity
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.778
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive predictive value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.829
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
negative predictive value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.875
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
f_meas
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.866
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other common performance metrics are summarized above. Accuracy represents the percentage of correct predictions. Descriptions for each can be found at this link.&lt;a href=&#34;#fn6&#34; class=&#34;footnoteRef&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The initial split of the data set into training/testing was done randomly so a replicate of the procedure would yield slightly different results. V-fold cross validation is a resampling technique that allows for repeating the process of splitting the data, training the model, and assessing the results many times from the same data set. Each stop in the CV process is annotated in the comments within the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create multiple split objects w/ vfold cross-validation resampling
set.seed(925)
hd_cv_split_objects &amp;lt;- heart_dataset_clean_tbl %&amp;gt;% vfold_cv(strata = Diagnosis_Heart_Disease)
hd_cv_split_objects&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits           id    
##    &amp;lt;named list&amp;gt;     &amp;lt;chr&amp;gt; 
##  1 &amp;lt;split [270/31]&amp;gt; Fold01
##  2 &amp;lt;split [270/31]&amp;gt; Fold02
##  3 &amp;lt;split [270/31]&amp;gt; Fold03
##  4 &amp;lt;split [271/30]&amp;gt; Fold04
##  5 &amp;lt;split [271/30]&amp;gt; Fold05
##  6 &amp;lt;split [271/30]&amp;gt; Fold06
##  7 &amp;lt;split [271/30]&amp;gt; Fold07
##  8 &amp;lt;split [271/30]&amp;gt; Fold08
##  9 &amp;lt;split [272/29]&amp;gt; Fold09
## 10 &amp;lt;split [272/29]&amp;gt; Fold10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#I want a big function that takes a split object and an id
make_cv_predictions_fcn &amp;lt;- function(split, id){
  #extract data for analysis set from split obj
  #prep(train) the recipe and return updated recipe
  #bake(apply) trained recipe to new data  
  analysis_tbl &amp;lt;- analysis(split)
  trained_analysis_recipe &amp;lt;- prep(the_recipe ,training = analysis_tbl)
  baked_analysis_data_tbl &amp;lt;- bake(trained_analysis_recipe, new_data = analysis_tbl)
  
  #define model in parsnip syntax
  model &amp;lt;- logistic_reg(mode = &amp;quot;classification&amp;quot;) %&amp;gt;%
    set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
    fit(Diagnosis_Heart_Disease ~ ., data = baked_analysis_data_tbl)
  
  #same as above but for assessment set (like the test set but for resamples)
  assessment_tbl &amp;lt;- assessment(split)
  trained_assessment_recipe &amp;lt;- prep(the_recipe, training = assessment_tbl)
  baked_assessment_data_tbl &amp;lt;- bake(trained_assessment_recipe, new_data = assessment_tbl)
  
  #make a tibble with the results
  tibble(&amp;quot;id&amp;quot;         = id,
         &amp;quot;truth&amp;quot;      = baked_assessment_data_tbl$Diagnosis_Heart_Disease,
         &amp;quot;prediction&amp;quot; = unlist(predict(model, new_data = baked_assessment_data_tbl))
  )
}

#map the big function to every split obj / id in the initial cv split tbl
cv_predictions_tbl &amp;lt;- map2_df(.x = hd_cv_split_objects$splits,
                              .y = hd_cv_split_objects$id,
                              ~make_cv_predictions_fcn(split = .x, id = .y))

#see results 
cv_predictions_tbl %&amp;gt;% head(10) %&amp;gt;% kable(align = rep(&amp;quot;c&amp;quot;, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
truth
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
prediction
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#define desired metrics
desired_metrics &amp;lt;- metric_set(accuracy,
                              sens,
                              spec,
                              ppv,
                              npv,
                              f_meas)

#group by fold and use get desired metrics [metric_set fcn is from yardstick]
cv_metrics_long_tbl &amp;lt;- cv_predictions_tbl %&amp;gt;% 
                       group_by(id) %&amp;gt;% 
                       desired_metrics(truth = truth, estimate = prediction) 

#see results
cv_metrics_long_tbl %&amp;gt;% head(10) %&amp;gt;% kable(align = rep(&amp;quot;c&amp;quot;, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
.metric
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
.estimator
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
.estimate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8709677
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9354839
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8387097
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.7666667
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.7333333
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.7931034
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fold10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.9310345
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#visualize results
cv_metrics_long_tbl %&amp;gt;% ggplot(aes(x = .metric, y = .estimate)) +
  geom_boxplot(aes(fill = .metric), 
               alpha = .6, 
               fatten = .7) +
  geom_jitter(alpha = 0.2, width = .05) +
  labs(x = &amp;quot;&amp;quot;,
       y = &amp;quot;&amp;quot;,
       title = &amp;quot;Boxplots for Logistic Regression&amp;quot;,
       subtitle = &amp;quot;Model Metrics, 10-Fold Cross Validation&amp;quot;) +
  scale_fill_viridis_d() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1) ) +
  theme(legend.title = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank()) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-29-heart-disease-prediction-from-patient-data-in-r_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;100%&#34; height=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calculate the mean from all the folds for each metric
cv_mean_metrics_tbl &amp;lt;- cv_metrics_long_tbl %&amp;gt;%
                       group_by(.metric) %&amp;gt;%
                       summarize(&amp;quot;Avg&amp;quot; = mean(.estimate)) %&amp;gt;%
                       ungroup()
  
cv_mean_metrics_tbl %&amp;gt;% 
  mutate(Average = Avg %&amp;gt;% signif(digits = 3)) %&amp;gt;% 
  select(.metric,
         Average) %&amp;gt;%
  kable(align = rep(&amp;quot;c&amp;quot;, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
.metric
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Average
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.837
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
f_meas
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.853
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
npv
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.852
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ppv
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.839
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
sens
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.876
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
spec
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.790
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Got there! Average of .837 accuracy after 10-fold cross-validation. Not bad for a basic logistic regression. It is certainly possible that .837 is not sufficient for our purposes given that we are in the domain of health care where false classifications have dire consequences. Evaluating other algorithms would be a logical next step for improving the accuracy and reducing patient risk.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Image Credit: Shutterstock/Crevis&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;UCI Machine Learning Repository, &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Heart+Disease&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Heart+Disease&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Nuclear stress testing requires the injection of a tracer, commonly technicium 99M (Myoview or Cardiolyte), which is then taken up by healthy, viable myocardial cells. A camera (detector) is used afterwards to image the heart and compare segments. A coronary stenosis is detected when a myocardial segment takes up the nuclear tracer at rest, but not during cardiac stress. This is called a “reversible defect.” Scarred myocardium from prior infarct will not take up tracer at all and is referred to as a “fixed defect.”&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data&lt;/a&gt;&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://notast.netlify.com/post/explaining-predictions-interpretable-models-logistic-regression/&#34; class=&#34;uri&#34;&gt;https://notast.netlify.com/post/explaining-predictions-interpretable-models-logistic-regression/&lt;/a&gt;&lt;a href=&#34;#fnref5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/&#34; class=&#34;uri&#34;&gt;https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/&lt;/a&gt;&lt;a href=&#34;#fnref6&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>